{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度检验\n",
    "\n",
    "欢迎来到本小节最后一个实战编程！在本次编程中你将加深对梯度检验的认识以及学会如何用代码实现梯度检验。\n",
    "\n",
    "假设你们公司有一个全球支付项目，老板要求你构建一个AI模型来判断支付是否可靠。\n",
    "\n",
    "由于反向传播的实现是有一定难度的，所以有时候会出现bug。因为支付项目对准确性要求很高，所以你的老板要求你百分之百保证你的反向传播是没有问题的。你老板说：“你要证明给我看，证明你的反向传播是完全正确的”。那么，这时，最好的证明手段就是我们前面文章中提到的梯度检验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCases import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 梯度检验的原理是什么？\n",
    "\n",
    "我们知道，反向传播可以计算出梯度。假设我们要计算$\\theta$相当于成本函数$J$的梯度$\\frac{\\partial J}{\\partial \\theta}$.\n",
    "\n",
    "因为前向传播是比较容易实现的，你也非常自信地确保你的前向传播是正确的，也就是说你百分之百地确信你的成本$J$是正确的。所以，你就可以用计算成本$J$的前向传播代码来验证计算梯度$\\frac{\\partial J}{\\partial \\theta}$的反向传播代码。\n",
    "\n",
    "我们回顾一下数学中导数（梯度）的定义：\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "如果你没有学过高数，不认识 \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\" 这个符号，没有关系，它的意思很简单，就是“当$\\varepsilon$很小很小，无限小，无限靠近0的时候”\n",
    "\n",
    "我们来捋一捋：\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ 是你通过反向传播计算得到的，你需要验证它是否准确\n",
    "- 所以我们就可以用另外一种方式计算出$\\frac{\\partial J}{\\partial \\theta}$，如果它与反向传播计算得到的一样，那么反向传播就是正确的。而这另外的计算方式就是我们上面的公式（1），我们可以用前向传播分别计算出$J(\\theta + \\varepsilon)$和$J(\\theta - \\varepsilon)$来求得$\\frac{\\partial J}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 1维的梯度检验\n",
    "\n",
    "为了便于大家理解，我们先来学懂简单的1维的梯度检验，后面再学N维的。\n",
    "\n",
    "假设我们有一个简单的1维线性函数$J(\\theta) = \\theta x$。这个函数（这个模型）只有一个参数$\\theta$；$x$是输入。\n",
    "\n",
    "下面我们会用代码来计算出$J(.)$（用前向传播计算出成本）然后计算出$\\frac{\\partial J}{\\partial \\theta}$（用反向传播计算出梯度）。最后我们用梯度检验来证明反向传播计算出来的梯度是正确的。\n",
    "\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> <u> **图 1** </u>: **1维线性模型**<br> </center></caption>\n",
    "\n",
    "上面的流程图显示出了关键的步骤：输入 $x$；然后计算出 $J(x)$（前向传播）；然后计算出梯度 $\\frac{\\partial J}{\\partial \\theta}$（反向传播）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "def forward_propagation(x, theta):\n",
    " \n",
    "    J = np.dot(theta, x)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "def backward_propagation(x, theta):\n",
    "    \n",
    "    # 这个函数的导数就是x，这是由微积分公式得来的，如果你没有学过微积分，没有关系，不用弄明白为什么。重点不在于此。\n",
    "    dtheta = x \n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们将用梯度检验来确认上面反向传播计算出来的梯度dtheta是正确的。\n",
    "\n",
    "**主要步骤如下**:\n",
    "- 我们先通过下面5小步来利用前向传播计算出梯度，这个梯度我们用gradapprox表示。\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- 然后利用上面的反向传播也计算出一个梯度，这个梯度我们用grad表示。\n",
    "- 最后，我们用下面的公式来计算gradapprox和grad这两个梯度相差多远。\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$   \n",
    "- 如果两个梯度相差小于$10^{-7}$，那么说明这两个梯度很接近，也就是说，你的反向传播是正确的；否则，说明你的反向传播里面有问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "\n",
    "    # 利用前向传播计算出一个梯度\n",
    "    thetaplus = theta + epsilon                              \n",
    "    thetaminus = theta - epsilon                         \n",
    "    J_plus = forward_propagation(x, thetaplus)              \n",
    "    J_minus = forward_propagation(x, thetaminus)            \n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)         \n",
    "    \n",
    "    # 利用反向传播也计算出一个梯度\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "    # 对比两个梯度相差多远\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                    \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)   \n",
    "    difference = numerator / denominator                              \n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print(\"反向传播是正确的!\")\n",
    "    else:\n",
    "        print(\"反向传播有问题！\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反向传播是正确的!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜，两个梯度相差小于$10^{-7}$ 。说明上面的反向传播得到的梯度是正确的。\n",
    "\n",
    "但是通常情况下，神经网络的成本函数不仅仅只有一个1维的参数。在神经网络模型中，$\\theta$通常是由多个$W^{[l]}$和$b^{[l]}$矩阵构成的。所以学会如何给多维参数做梯度检验是很重要的。下面我们就来学习多维参数的梯度检验！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 多维梯度检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "下图展示了你的支付可靠度预测模型的前向传播和反向传播流程。\n",
    "\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **图 2** </u>: **神经网络流程图**<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*</center></caption>\n",
    "\n",
    "下面是你的前向传播和反向传播的代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1. / m * np.sum(logprobs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2  # ~~\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True) # ~~\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "但是上面的反向传播实现得正确吗？下面我们将用梯度检验来验证它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何进行多维梯度检验**.\n",
    "\n",
    "像1维检验时一样，我们还是使用下面的这个公式:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "但是，多维检验中的$\\theta$不再是一个数值了，而是一个字典了，字典里面包含了很多个参数。我给大家实现了一个函数\"`dictionary_to_vector()`\"，用它可以将这个字典转换成一个向量，它会改变字典里参数(W1, b1, W2, b2, W3, b3)的维度并且将它们连接起来构成一个大向量，这个向量我们用\"values\"来表示。\n",
    "\n",
    "同时也给大家提供了另外一个逆操作的函数\"`vector_to_dictionary`\"，它会将向量转换回字典形式。\n",
    "\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **图 2** </u>: **dictionary_to_vector() 和 vector_to_dictionary()**<br>在后面的 gradient_check_n()函数中你将使用它们来进行梯度验证</center></caption>\n",
    "\n",
    "当然，我们也会将梯度\"gradients\"对应的字典也转换成一个大向量，转换函数是gradients_to_vector()。这些函数我都为大家写好了，大家不用担心，把精力放在重点要理解的理论上就行了。\n",
    "\n",
    "为了便于大家理解下面的梯度检验函数gradient_check_n()。我先给出它的伪码实现，这有助于大家理解它的逻辑。\n",
    "\n",
    "For each i in num_parameters:\n",
    "- To compute `J_plus[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- To compute `J_minus[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "上面的循环完了后，你就通过前向传播的方式获得了梯度gradapprox，它的每一个元素gradapprox[i]就对应着每一个参数`parameter_values[i]`相关的梯度。然后你再用1维梯度检验时相同的方式来对比梯度gradapprox和反向传播得到的梯度相差远不远。对比公式还是下面这个。\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n",
    "     \n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    # 计算gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        thetaplus =  np.copy(parameters_values)                                      \n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                 \n",
    "        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  \n",
    "        \n",
    "        thetaminus = np.copy(parameters_values)                                     \n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                                      \n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) \n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)                                \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)               \n",
    "    difference = numerator / denominator                                         \n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print(\"\\033[93m\" + \"反向传播有问题! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\"\\033[92m\" + \"反向传播很完美! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m反向传播有问题! difference = 0.285093156780699\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哈哈，梯度检验发现我们上面的反向传播有问题。这是我故意将错误放入`backward_propagation_n`函数的。给你们留了个作业，你们自己检查`backward_propagation_n`函数，看错误在哪里（提示：检查dW2和db1）。找到错误，并且更正后，再次运行梯度检验看看结果。注意：更改了`backward_propagation_n`后，一定要重新运行它所在的notebook单元。\n",
    "\n",
    "**注意** \n",
    "\n",
    "- 梯度检验是很缓慢的。通过$\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$来计算梯度非常消耗计算力。所以，我们不会在训练的每一个回合都执行梯度检验。仅仅偶尔执行几次。\n",
    "- 梯度检验是无法与dropout共存的。所以在执行梯度检验时，要把dropout关掉，检验完毕后再开启。\n",
    "\n",
    "恭喜，你现在已经能百分之百坚信自己的反向传播是没有问题的了！你可以用梯度检验证明给你的老板看，你的代码没有问题！\n",
    "\n",
    "<font color='blue'>\n",
    "**本次实战编程需要记住的几点**:\n",
    "\n",
    "- 梯度检验通过用前向传播的方式求出一个梯度，然后将其与反向传播求出的梯度进行对比来判断梯度是否正确\n",
    "\n",
    "- 梯度检验很浪费计算力。所以只在需要验证代码是否正确时才开启。确认代码没有问题后，就关闭掉梯度检验。 "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
