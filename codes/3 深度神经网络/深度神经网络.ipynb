{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实战我们构建一个深度神经网络，并用它来完成和《第一个人工智能程序》时同样的任务——识别图片中是否有猫。在《第一个人工智能程序》时，我们使用的是单神经元神经网络，本次我们使用深度神经网络来提升预测精准度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载我们自定义的工具函数\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "# 设置一些画图相关的参数\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将构建深度神经网络所需的工具函数一个个给编写好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数用于初始化所有层的参数w和b\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    layer_dims -- 这个list列表里面，包含了每层的神经元个数。\n",
    "    例如，layer_dims=[5,4,3]，表示第一层有5个神经元，第二层有4个，最后一层有3个神经元\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 这个字典里面包含了每层对应的已经初始化了的W和b。\n",
    "    例如，parameters['W1']装载了第一层的w，parameters['b1']装载了第一层的b\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # 获取神经网络总共有几层\n",
    "\n",
    "    # 遍历每一层，为每一层的W和b进行初始化\n",
    "    for l in range(1, L):\n",
    "        # 构建并随机初始化该层的W。由我前面的文章《1.4.3 核对矩阵的维度》可知，Wl的维度是(n[l] , n[l-1]) \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])\n",
    "        # 构建并初始化b\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # 核对一下W和b的维度是我们预期的维度\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    #就是利用上面的循环，我们就可以为任意层数的神经网络进行参数初始化，只要我们提供每一层的神经元个数就可以了。       \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始构建前向传播所需的工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_forward用于实现公式 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$，这个称之为线性前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):   \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b) # 将这些变量保存起来，因为后面进行反向传播时会用到它们\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_forward用于实现公式 $A^{[l]} = g(Z^{[l]})$，g代表激活函数，使用了激活函数之后上面的线性前向传播就变成了非线性前向传播了。在dnn_utils.py中我们自定义了两个激活函数，sigmoid和relu。它们都会根据传入的Z计算出A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A。第一层时A_prev就是特征输入X\n",
    "    W -- 本层相关的W\n",
    "    b -- 本层相关的b\n",
    "    activation -- 两个字符串，\"sigmoid\"或\"relu\"，指示该层应该使用哪种激活函数\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\": # 如果该层使用sigmoid        \n",
    "        A = sigmoid(Z) \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, Z) # 缓存一些变量，后面的反向传播会用到它们\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数构建了一个完整的前向传播过程。这个前向传播一共有L层，前面的L-1层用的激活函数是relu，最后一层使用sigmoid。\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    X -- 输入的特征数据\n",
    "    parameters -- 这个list列表里面包含了每一层的参数w和b\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    # 获取参数列表的长度，这个长度的一半就是神经网络的层数。\n",
    "    # 为什么是一半呢？因为列表是这样的[w1,b1,w2,b2...wl,bl],里面的w1和b1代表了一层\n",
    "    L = len(parameters) // 2  \n",
    "    \n",
    "    # 循环L-1次，即进行L-1步前向传播，每一步使用的激活函数都是relu\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)],\n",
    "                                             activation='relu')\n",
    "        caches.append(cache)# 把一些变量数据保存起来，以便后面的反向传播使用\n",
    "        \n",
    "    \n",
    "    # 进行最后一层的前向传播，这一层的激活函数是sigmoid。得出的AL就是y'预测值\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面已经完成了前向传播了。下面这个函数用于计算成本（单个样本时是损失，多个样本时是成本）。\n",
    "# 通过每次训练的成本我们就可以知道当前神经网络学习的程度好坏。\n",
    "def compute_cost(AL, Y):\n",
    "       \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)# 确保cost是一个数值而不是一个数组的形式\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面已经实现了前向传播和成本函数，下面开始实现反向传播。通过反向传播来计算梯度——计算每层的w和b相当于成本函数的偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_backward函数用于根据后一层的dZ来计算前面一层的dW，db和dA。也就是实现了下面3个公式\n",
    "$$ dW^{[l]}  = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dZ -- 后面一层的dZ\n",
    "    cache -- 前向传播时我们保存下来的关于本层的一些变量\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_backward用于根据本层的dA计算出本层的dZ。就是实现了下面的公式\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "上式的g'()表示求Z相当于本层的激活函数的偏导数。所以不同的激活函数也有不同的求导公式。\n",
    "我们为大家编写了两个求导函数sigmoid_backward和relu_backward。大家当前不需要关心这两个函数的内部实现，当然，如果你感兴趣可以到dnn_utils.py里面去看它们的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dA -- 本层的dA \n",
    "    cache -- 前向传播时保存的本层的相关变量\n",
    "    activation -- 指示该层使用的是什么激活函数: \"sigmoid\" 或 \"relu\"\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    # 这里我们又顺带根据本层的dZ算出本层的dW和db以及前一层的dA\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面这个函数构建出整个反向传播。\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    AL -- 最后一层的A，也就是y'，预测出的标签\n",
    "    Y -- 真实标签\n",
    "    caches -- 前向传播时保存的每一层的相关变量，用于辅助计算反向传播\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # 获取神经网络层数。caches列表的长度就等于神经网络的层数\n",
    "    Y = Y.reshape(AL.shape) # 让真实标签的维度和预测标签的维度一致\n",
    "    \n",
    "    # 计算出最后一层的dA，前面文章我们以及解释过，最后一层的dA与前面各层的dA的计算公式不同，\n",
    "    # 因为最后一个A是直接作为参数传递到成本函数的，所以不需要链式法则而直接就可以求dA（A相当于成本函数的偏导数）\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # 计算最后一层的dW和db，因为最后一层使用的激活函数是sigmoid\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "                                                                                            dAL, \n",
    "                                                                                            current_cache,\n",
    "                                                                                            activation = \"sigmoid\")\n",
    "\n",
    "    # 计算前面L-1层到第一层的每层的梯度，这些层都使用relu激活函数\n",
    "    for c in reversed(range(1,L)): # reversed(range(1,L))的结果是L-1,L-2...1。是不包括L的。第0层是输入层，不必计算。\n",
    "        # 这里的c表示当前层\n",
    "        grads[\"dA\" + str(c-1)], grads[\"dW\" + str(c)], grads[\"db\" + str(c)] = linear_activation_backward(\n",
    "            grads[\"dA\" + str(c)], \n",
    "            caches[c-1],\n",
    "            # 这里我们也是需要当前层的caches，但是为什么是c-1呢？因为grads是字典，我们从1开始计数，而caches是列表，\n",
    "            # 是从0开始计数。所以c-1就代表了c层的caches。数组的索引很容易引起莫名其妙的问题，大家编程时一定要留意。\n",
    "            activation = \"relu\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的反向传播，我们得到了每一层的梯度（每一层w和b相当于成本函数的偏导数）。下面的update_parameters函数将利用这些梯度来更新/优化每一层的w和b，也就是进行梯度下降。\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- 每一层的参数w和b \n",
    "    grads -- 每一层的梯度\n",
    "    learning_rate -- 是学习率，学习步进\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # 获取层数。//除法可以得到整数\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print (\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print (\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print (\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，工具函数都编写好了，下面我们开始使用它们来构建一个深度神经网络以识别图片中是否有猫。\n",
    "首先，我们还是老规矩——第一步加载数据集。本次的数据集和《第一个人工智能程序》时用的是一样一样的。\n",
    "关于数据集的信息和加载方式可以看我的《第一个人工智能程序》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "m_train = train_x_orig.shape[0] # 训练样本的数量\n",
    "m_test = test_x_orig.shape[0] # 测试样本的数量\n",
    "num_px = test_x_orig.shape[1] # 每张图片的宽/高\n",
    "\n",
    "# 为了方便后面进行矩阵运算，我们需要将样本数据进行扁平化和转置\n",
    "# 处理后的数组各维度的含义是（图片数据，样本数）\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T \n",
    "\n",
    "# 下面我们对特征数据进行了简单的标准化处理（除以255，使所有值都在[0，1]范围内）\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用上面的工具函数构建一个深度神经网络训练模型\n",
    "def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n",
    "    \"\"\"    \n",
    "    参数:\n",
    "    X -- 数据集\n",
    "    Y -- 数据集标签\n",
    "    layers_dims -- 指示该深度神经网络用多少层，每层有多少个神经元\n",
    "    learning_rate -- 学习率\n",
    "    num_iterations -- 指示需要训练多少次\n",
    "    print_cost -- 指示是否需要在将训练过程中的成本信息打印出来，好知道训练的进度好坏。\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 返回训练好的参数。以后就可以用这些参数来识别新的陌生的图片\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                  \n",
    "\n",
    "    # 初始化每层的参数w和b\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # 按照指示的次数来训练深度神经网络\n",
    "    for i in range(0, num_iterations):\n",
    "        # 进行前向传播\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        # 计算成本\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # 进行反向传播\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # 更新参数，好用这些参数进行下一轮的前向传播\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # 打印出成本\n",
    "        if i % 100 == 0:\n",
    "            if print_cost and i > 0:\n",
    "                print (\"训练%i次后成本是: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # 画出成本曲线图\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练100次后成本是: 0.672053\n",
      "训练200次后成本是: 0.648263\n",
      "训练300次后成本是: 0.611507\n",
      "训练400次后成本是: 0.567047\n",
      "训练500次后成本是: 0.540138\n",
      "训练600次后成本是: 0.527930\n",
      "训练700次后成本是: 0.465477\n",
      "训练800次后成本是: 0.369126\n",
      "训练900次后成本是: 0.391747\n",
      "训练1000次后成本是: 0.315187\n",
      "训练1100次后成本是: 0.272700\n",
      "训练1200次后成本是: 0.237419\n",
      "训练1300次后成本是: 0.199601\n",
      "训练1400次后成本是: 0.189263\n",
      "训练1500次后成本是: 0.161189\n",
      "训练1600次后成本是: 0.148214\n",
      "训练1700次后成本是: 0.137775\n",
      "训练1800次后成本是: 0.129740\n",
      "训练1900次后成本是: 0.121225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bafb8c7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置好深度神经网络的层次信息——下面代表了一个4层的神经网络（12288是输入层），\n",
    "# 第一层有20个神经元，第二层有7个神经元。。。\n",
    "# 你也可以构建任意层任意神经元数量的神经网络，只需要更改下面这个数组就可以了\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# 根据上面的层次信息来构建一个深度神经网络，并且用之前加载的数据集来训练这个神经网络，得出训练后的参数\n",
    "parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们再编写一个预测函数，来用上面训练得到的参数来进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):   \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # 进行一次前向传播，得到预测结果\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "   \n",
    "    # 将预测结果转化成0和1的形式，即大于0.5的就是1，否则就是0\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率是: 0.9808612440191385\n"
     ]
    }
   ],
   "source": [
    "# 对训练数据集进行预测\n",
    "pred_train = predict(train_x,parameters)\n",
    "print(\"预测准确率是: \"  + str(np.sum((pred_train == train_y) / train_x.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率是: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 对测试数据集进行预测\n",
    "pred_test = predict(test_x,parameters)\n",
    "print(\"预测准确率是: \"  + str(np.sum((pred_test == test_y) / test_x.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，比我们之前的的单神经元网络的0.7提升了0.1。不要小看这0.1，这是很难提升的。别说是0.1，如果全世界都只能做到0.9，如果你能提升0.05，那么你就享誉全球啦！\n",
    "\n",
    "当然，0.8还不是我们的极限，后面的文章我会带领大家继续提升它。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
