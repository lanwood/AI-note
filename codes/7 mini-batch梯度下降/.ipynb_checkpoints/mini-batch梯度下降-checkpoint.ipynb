{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-batch梯度下降\n",
    "\n",
    "之前，我们在实战编程中一直都是用梯度下降算法来更新参数来使成本函数最小化。本次实战编程中我们将使用更高级的优化算法来提升神经网络的学习效率。这些优化算法的使用可能会让你更加靠近最小值处。一个好的优化算法可以大大加速神经网络的训练时间，以前需要几天时间来训练的神经网络可能几个小时就搞定了。\n",
    "\n",
    "梯度下降找最小值的过程就像下图一样：\n",
    "<img src=\"images/cost.jpg\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u> **图 1** </u>: **找最小值处就像找山谷的谷底一样**<br> 在梯度下降的每一步中，都会朝着某个方向来更新参数以找到成本函数的最小值处。</center></caption>\n",
    "\n",
    "**注意**: 数学中的偏导数符号$\\frac{\\partial J}{\\partial a }在编程中我们用$ `da`来表示.\n",
    "\n",
    "好，让我们开始本次的实战编程吧！\n",
    "\n",
    "首先还是要加载一些系统工具库以及一些我们自定义的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\InstalledFiles\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from testCases import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 梯度下降\n",
    "\n",
    "之前我们一直使用的梯度下降算法只是一个简单的基础优化算法。如果每次梯度下降的学习对象都是所有的样本，那么这个梯度下降算法就叫做Batch梯度下降。\n",
    "\n",
    "下面这个就是梯度下降中用来更新参数的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # 获取神经网络的层数。这里除以2是因为字典里面包含了w和b两种参数。\n",
    "\n",
    "    # 遍历每一层\n",
    "    for l in range(L):\n",
    "        # 下面使用l + 1，是因为l是从0开始的，而我们的参数字典是从1开始的\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.63535156 -0.62320365 -0.53718766]\n",
      " [-1.07799357  0.85639907 -2.29470142]]\n",
      "b1 = [[ 1.74604067]\n",
      " [-0.75184921]]\n",
      "W2 = [[ 0.32171798 -0.25467393  1.46902454]\n",
      " [-2.05617317 -0.31554548 -0.3756023 ]\n",
      " [ 1.1404819  -1.09976462 -0.1612551 ]]\n",
      "b2 = [[-0.88020257]\n",
      " [ 0.02561572]\n",
      " [ 0.57539477]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与batch梯度下降相对的另一个极端是随机梯度下降。当mini-batch梯度下降的子数据集中只有1个样本时，就是随机梯度下降。这3种梯度下降算法的参数更新算法都是一样的（都是用上面我们实现的更新函数）。不同的是，随机梯度下降每次的学习对象只有一个样本，而batch梯度下降每次的学习对象是所有样本。下面的两个代码段展示了batch梯度下降和随机梯度下降的差别。\n",
    "\n",
    "- **batch梯度下降**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(a, Y)\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "- **随机梯度下降**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m): # 遍历循环每一个样本\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在随机梯度下降中，每次都只使用一个样本来进行梯度下降。如果你的数据集非常大，那么使用随机梯度下降可能会比batch梯度下降更快。但是随机梯度下降的方向会很不稳定。如下图所示，左图是随机梯度下降，右图是batch梯度下降。\n",
    "\n",
    "<img src=\"images/kiank_sgd.png\" style=\"width:750px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **图 1** </u><font color='purple'>  : **随机梯度下降 vs batch梯度下降**<br> </center></caption>\n",
    "\n",
    "另外有些同学应该也注意到了，随机梯度下降要使用3个循环\n",
    "1. 遍历每一次梯度下降iteration。\n",
    "2. 遍历所有样本。\n",
    "3. 遍历神经网络的每一层。\n",
    "\n",
    "在实际编程中，往往使用mini-batch梯度下降会比batch梯度下降和随机梯度下降都要高效，使神经网络学习得更快。下面两个图展示了随机梯度下降和mini-batch梯度下降的学习路径。\n",
    "\n",
    "<img src=\"images/kiank_minibatch.png\" style=\"width:750px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **图 2** </u>: <font color='purple'>  **随机梯度下降 vs mini-batch梯度下降**<br> </center></caption>\n",
    "\n",
    "<font color='blue'>\n",
    "**下面两点大家需要牢记**:\n",
    "- 这3个梯度下降的区别仅仅在于它们每次学习的样本数量不同。\n",
    "- 无论是哪种梯度下降，学习率都是必须要精心调的。\n",
    "- 通常来说，如果数据集很大，那么mini-batch梯度下降会比另外2种要高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - mini-batch梯度下降\n",
    "\n",
    "下面我们来学习如何从训练集(X, Y)中创建mini-batch\n",
    "\n",
    "分两个步骤:\n",
    "- **洗牌**: 如下图所示，将训练集(X, Y)进行洗牌——将样本随机调换位置。这样一来，每一次的子训练集中都包含着不同的样本。下图中每一列就代表了一个训练样本。注意，在洗牌时，X和Y是被绑在一起进行洗牌的，也就是说，在洗牌后，之前第$i^{th}$列中的X还是与之前$i^{th}$列的Y是一对。\n",
    "\n",
    "<img src=\"images/kiank_shuffle.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "- **分割**: 将洗牌后的训练集划分为一个个小的子训练集。这里我们每个子训练集中有64个样本，也就是说mini_batch_size是64。注意，有时候训练集无法被mini_batch_size整除，那么最后一个子训练集里面的样本数就会小于mini_batch_size。这个没有关系的。 \n",
    "\n",
    "<img src=\"images/kiank_partition.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "下面的函数实现了上面两步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "       \n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[1]  # 获取样本数量\n",
    "    mini_batches = []\n",
    "        \n",
    "    # 第一步: 洗牌训练集\n",
    "    permutation = list(np.random.permutation(m)) # 这行代码会生成m范围内的随机整数，如果m是3，那么结果可能为[2, 0, 1]\n",
    "    shuffled_X = X[:, permutation]# 这个代码会将X按permutation列表里面的随机索引进行洗牌。为什么前面是个冒号，因为前面是特征，后面才代表样本数 \n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # 第二步: 分割洗牌后的训练集\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # 获取子训练集的个数（不包括后面不满mini_batch_size的那个子训练集）\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # 出来后面不满mini_batch_size的那个子训练集\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个mini_batch_X的维度: (12288, 64)\n",
      "第二个mini_batch_X的维度: (12288, 64)\n",
      "第三个mini_batch_X的维度: (12288, 20)\n",
      "第一个mini_batch_Y的维度: (1, 64)\n",
      "第二个mini_batch_Y的维度: (1, 64)\n",
      "第三个mini_batch_Y的维度: (1, 20)\n"
     ]
    }
   ],
   "source": [
    "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "\n",
    "print(\"第一个mini_batch_X的维度: \" + str(mini_batches[0][0].shape))\n",
    "print(\"第二个mini_batch_X的维度: \" + str(mini_batches[1][0].shape))\n",
    "print(\"第三个mini_batch_X的维度: \" + str(mini_batches[2][0].shape))\n",
    "print(\"第一个mini_batch_Y的维度: \" + str(mini_batches[0][1].shape))\n",
    "print(\"第二个mini_batch_Y的维度: \" + str(mini_batches[1][1].shape)) \n",
    "print(\"第三个mini_batch_Y的维度: \" + str(mini_batches[2][1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**大家需要记住下面几点**:\n",
    "- 洗牌和分割是实现mini-batch梯度下降的两个重要步骤。\n",
    "- mini-batch的大小一般选择2的次方，16，32，64，128..."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "Ckiv2",
   "launcher_item_id": "eNLYh"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
